{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "Here we will learn how to explore datasets which depend on time. As you might imagine, many datasets in the real world are timeseries. The stock market springs to mind, but also anything to do with sales or marketing, engineering processes (when will this particular turbine break, you may ask?), medical processes (what is the effect of this medication over time), and so many, many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas for Timeseries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this BLU we will not learn any fancy prediction stuff, but rather how to wrangle timeseries data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np \n",
    "import matplotlib \n",
    "\n",
    "\n",
    "np.random.seed(1000)\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = [16, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timestamp is the most basic form of time series indexer that Pandas has. It does exactly what the name describes: marks the exact moment in which the data was collected. \n",
    "\n",
    "An event happens, and the time of the event is dumped into a database. \n",
    "\n",
    "One example of this would be... bitcoin! Now, whatever you may think about bitcoin, ( _whether it is a ponzi scheme or a perfectly legitimate way to destroy the environment while helping organ traffickers and kidnappers launder money_ ), it is an excellent source of high-granularity data. Let's dive in! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'https://raw.githubusercontent.com/vohcolab/PandaViz-Workshop/main/Pandas/Time%20Series/data/bitcoin.csv'\n",
    "# data_path = '../data/bitcoin.csv' # also works if you are running this notebook locally\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. We have this `Timestamp` column, that we can kind of parse by looking at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Timestamp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can kind of understand this. Looks like Year, month, and day, then hours, minutes, then seconds. But the dtype is just object, how boring.\n",
    "\n",
    "However, pandas can do something pretty amazing with these objects: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_as_a_timestamp = pd.to_datetime(data.Timestamp, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is it now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_as_a_timestamp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a `datetime64[ns]`, which I shall for the sake of simplicity just refer to as a TimeStamp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_as_a_timestamp.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_as_a_timestamp.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do with this? Well, for one thing, extracting days, months etc is trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_as_a_timestamp.dt.day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the nomenclature. `Series.dt.<whatever I want>`. \n",
    "\n",
    "And we can want [just about anything we can think of!](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-date-components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll make a little dataset so that we can see some of the results side by side\n",
    "new = pd.DataFrame()\n",
    "new['date'] = time_as_a_timestamp\n",
    "new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new['day'] = new['date'].dt.day\n",
    "new['month'] = new['date'].dt.month\n",
    "new['year'] = new['date'].dt.year\n",
    "new['hour'] = new['date'].dt.hour\n",
    "new['minute'] = new['date'].dt.minute\n",
    "new['second'] = new['date'].dt.second\n",
    "new['day of the week'] = new['date'].dt.weekday\n",
    "new['day of the week name'] = new['date'].dt.day_name()\n",
    "new['quarter'] = new['date'].dt.quarter\n",
    "new['is it a leap year?'] = new['date'].dt.is_leap_year\n",
    "\n",
    "new.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas... is amazing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, back to our data. Let's try to ask some useful questions, such as \n",
    "> \" _How has the price of bitcoin varied over time?_ \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by making the timestamp the index. This is common good practice, for reasons we shall soon see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Timestamp = pd.to_datetime(data.Timestamp, infer_datetime_format=True, yearfirst=True)\n",
    "\n",
    "data = data.set_index('Timestamp',    # <---- Set the index to be our timestamp data  \n",
    "                      drop=True)      # <---- drop the original column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have a datetime index, you should always, always, always sort it! \n",
    "\n",
    "( _Note: I **deliberately won't remind you to do this in the exercises**, and if you forget, you will get wrong answers!_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have data between %s and %s' % (data.index.min(), data.index.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know that somewhere about [Jan 17th, bitcoin had a slight crash](https://www.cnbc.com/2018/01/17/bitcoin-tests-important-price-level-after-dramatic-plunge.html). Let's try to select that time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['Jan 17th 2018'].head()   # <--- wait, you can do that???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool huh? Now that we have a datetime index, we can do some crazy selecting, including just writing dates in that way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['Jan 17th 2018'].Close.plot(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select less specific date ranges. How's January? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['Jan 2018'].Close.plot(); # <--- Pandas... is... awesome "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see those days between the 15th and the 22nd. Let's select in a different way, for the sake of giggles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['01/15/2018':'01/22/2018'].Close.plot();  # <--- remember, American dates are less error prone in Pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. What were things like during that \"drop\"? From our first chart, we saw it was on the 17th, between 3PM and 4PM. Let's create a slice, so that we can access this interval without having to write every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = slice('01/17/2018 1:30PM','01/17/2018 4:30PM') # yep, minutes, seconds, up to nanoseconds actually! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[interval].Close.plot();  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did traders react? Let's get the volume "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby for timeseries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... Let's get our volume per minute! For this, we can use [resample](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwi3jfnKgNnaAhUGvBQKHRCwBd4QFggpMAA&url=https%3A%2F%2Fpandas.pydata.org%2Fpandas-docs%2Fstable%2Fgenerated%2Fpandas.DataFrame.resample.html&usg=AOvVaw1le9agxvLanaQp9zlNYG9Y)\n",
    "\n",
    "In our case, we'll resample to every 5 minutes, and take the sum (because we want to sum the volume of those 5 minutes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[interval]['Volume_(Currency)'].resample('5 min').sum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! \n",
    "\n",
    "Here's a question: How much money (in dollars) was traded in the largest 10 minutes peak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money = data.loc[interval]['Volume_(Currency)'].resample('10 min').sum().max()\n",
    "\n",
    "print('In 10 minutes, %0.1f dollars were traded in Bitcoin' % money)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when did it happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[interval]['Volume_(Currency)'].resample('10 min').sum().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just took the sum, but what about if we were looking at prices, would that make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.resample('W').Close.mean().plot();  # the mean weekly closing prices, since 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time... is... cool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to know the total amount of money that has been traded in bitcoin? \n",
    "\n",
    "One way would simply be to sum it, but that doesn't give us any idea of how that total varied over time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Volume_(Currency)'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cooler way to see this over time is to use the cumulative sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Volume_(Currency)'].cumsum().plot();  # the total volume traded since the start "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've learnt before, there are many cool [methods for groupby](https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more important question however may be \n",
    "> \" _what were the biggest variations in price?_ \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we might find it useful to calculate consecutive differences between periods, using [diff](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.diff.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Close.diff().head()  # this can take a few seconds to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first entrance is NaN, which makes sense because it's got no previous day to subtract. \n",
    "\n",
    "What do the diffs look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Close.diff().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not particularly useful. How about on a particular day? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['May 22nd 2017'].Close.diff(periods=1).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our subsetting tools that we learned in the previous unit, and apply them here!\n",
    "\n",
    "Let's find if there were any periods in may 22 after where the price went up by 20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_22_diff = data.loc['May 22nd 2017'].Close.diff(periods=1)\n",
    "over20_mask = may_22_diff > 20 # vector of Trues and Falses\n",
    "may_22_diff[over20_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful groupby method is [rolling windows](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html). They do what their name suggest: aggregate the previous X periods using a certain function, for instance the mean. They are very useful to **smooth** choppy timeseries and be less reactive to noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say it's December 18th, in the early morning, and we are at our terminal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Midnight and a bit... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['Dec 18th 2017 00:08:00':'Dec 18th 2017 00:12:00', 'Weighted_Price'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgflip.com/29iucd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A few minutes pass... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['Dec 18th 2017 00:12:00':'Dec 18th 2017 00:15:00', 'Weighted_Price'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.redditmedia.com/VE5dgdjQ8FKZ47gdxJdQ07q36bsZVyhvAmllvLdtTnI.jpg?w=534&s=ce869cd0d8630cd420af7fa72b3c296d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A few more minutes... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['Dec 18th 2017 00:15:00':'Dec 18th 2017 00:18:00', 'Weighted_Price'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgflip.com/29iucd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think you get the picture. What's going on is that **we're being extremely reactive to noise**, and missing the underlying process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first hour of Dec 18th 2017, as seen by traders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['Dec 18th 2017 00:00:00':'Dec 18th 2017 01:00:00', 'Weighted_Price'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first hour of Dec 18th 2017, as seen by a rolling window of 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just the raw data, so we can apply a rolling window on it  \n",
    "first_hour = data.loc['Dec 18th 2017 00:00:00':'Dec 18th 2017 01:00:00', 'Weighted_Price']\n",
    "\n",
    "# notice the window size as a parameter of rolling, feel free to mess around with that parameter.\n",
    "# Also notice how we use the mean. We can use many others. Try changing it! \n",
    "window_size = 10\n",
    "first_hour_rolling_window = first_hour.rolling(window=window_size).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot these together \n",
    "first_hour_rolling_window.plot(color='b', label='rolling_window = %0.0f min' % window_size);\n",
    "first_hour.plot(label='raw data', alpha=.7, ls='-', color='orange');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the rolling mean appears as a smoother version of the original signal, this can be a great help when looking at noisy signals and you just want to get an idea of the overall trend. Also note that when doing the rolling mean you get a delayed version of the original signal, due to the nature of how it is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( _Note: In case you are curious about what would have happened if you had interpreted the yellow line as a potential recovery... It doesn't end well._ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's ask some more questions of this dataset! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the entire  dataset. Let's answer the following question:\n",
    "> What was the weekly change in price, over time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to weekly, get the mean Close price (per week), calculate the differences, and plot them \n",
    "data.resample('W').Close.mean().diff(periods=1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are observing something that makes sense. As the magnitude gets bigger, so does the volatility. It makes more sense for bitcoin to go down \\\\$100 in a week when it was \\\\$5000 than when it was at \\\\$200. \n",
    "\n",
    "What we actually want... is the percent change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to weekly, take the mean close price (weekly), and calcualte the percentage change \n",
    "data.resample('W').Close.mean().pct_change().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we've covered a number of methods for dealing with Timeseries. \n",
    "\n",
    "Next up, we go and try to examine the full stock market. Only to find, to our dismay, that we need multi-indexing. [What is multi-indexing, I hear you ask?](https://www.youtube.com/watch?v=gC24hhNbXN0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary of the methods we have learned:**\n",
    "* `pd.to_datetime()` - this allows you to create datetime format and gives you access to several methods that pandas has specifically to handle dates \n",
    "* We need to have a datetime sorted index!!\n",
    "* Selection: `dataframe.loc['Jan 17th 2018']` to select the 17th of January, 2018 \n",
    "    * Remember pandas is really helpful here as you can even do something like `dataframe.loc['Jan 2018']` or even ranges `dataframe.loc['01/15/2018':'01/22/2018']`\n",
    "    * If you're going to analyse the same interval several times, then using slice() can help a lot.\n",
    "*  How to Group by in time series. Don't forget that after grouping the data we need an aggregation function! \n",
    " * **resample**, e.g: `.resample('5min').sum()`.\n",
    " * **Rolling window**: `.rolling()`\n",
    "    * few new aggregation functions that we've talked: \n",
    "        * `sum()`\n",
    "        * `cumsum()`\n",
    "        * `cummax()`\n",
    "        * `max()`\n",
    "        * ...\n",
    "* We have also learnt how to calculate the difference between periods, using `diff()` and `pct_change`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
